# 剪枝比例

统计各模块参数量以评估可剪枝比例。 获取上一命令输出（使用默认活动终端 ID）。 任务理解：需要判断 lsk_s_fpn_1x_dota_le90 中哪些层/结构适合做剪枝，并评估是否能达到总参数 10%~20% 的可剪空间。下面按模块给出可剪性、影响与大致收益估算思路（无需先改动输出通道即可达到目标）。

## 1. 各大模块剪枝友好度（从易到难）

1) ROI Head (RotatedShared2FCBBoxHead)

- 最大“肥肉”。fc1: 256*7*7*1024 ≈ 12.85M 参数，fc2: 1024*1024 ≈ 1.05M。两层合计 ~14M。
- 适合手动降维：把 fc_out_channels 从 1024→768 或 640，不动前面 256×7×7 的输入。
- 预计节省：
  - 1024→768：fc1 降 25%，fc2 降 44%，总省约 3.7M（若全模型 ~40M，相当于 ~9%）。
  - 1024→640：fc1 再降 (640/1024=62.5%)，fc2 变 640^2=0.41M，总省 ~5.6M（~14%）。精度影响可通过再训练弥补。
- 建议首选：单独降 fc_out_channels，简单、收益大、对预训练无破坏。

2) Backbone 内部 Block 的 MLP 隐藏维 (mlp_ratio)

- 现有 mlp_ratios = [8,8,4,4]。MLP 参数主要由 2*D^2*ratio 组成（DWConv 比例小）。
- 下调策略： [8,8,4,4] → [6,6,3,3]（或更保守 [7,7,4,3]）。
- 粗估：对前两 stage（D=64,128）影响较小；对 D=320,512 的阶段（ratio 4→3）可各省 ~25% 的 MLP 部分；四个 stage 总和再给 ~5%~7% 级别的全模型削减。
- 优点：不改变 stage 输出通道（FPN / 预训练兼容性较好，只需丢弃多余权重或插值载入，写自定义加载 mapping）。
- 实现方式：改配置 mlp_ratios，并自定义加载预训练时跳过形状不匹配层（init_cfg + strict=False）。

3) ROI Head 改两 FC 为 1FC 或 Conv Head

- 方案：去掉第二个 FC（或把两层 1024→(768→768) 改 768→分类+回归 heads）。
- 如果直接改为单 FC：保留 fc1 输出 C=768，然后分两头小线性层。再训练可恢复精度。收益与上一步可叠加。
- 注意：删除结构需更新权重初始化，避免复用不兼容预训练 fc2。

4) FPN out_channels 下调

- 256 → 224 / 192。联动修改：FPN(out_channels)、RPN(in_channels)、RoIExtractor(out_channels)、BBox Head in_channels、RPN/Head 中 conv 定义。
- 好处：触发“级联节省”——RPN conv、FPN extra conv、ROI Head 输入维度（fc1 的输入 变 (C *7*7)）一起减少。
- 成本：改动范围大；需统一改配置多个字段；已有 checkpoints 直接加载困难。
- 适合作为第二阶段（先做 Head+MLP，再全网宽度缩减）。

5) LSKblock 内部 channel 分流宽度

- 当前 conv1/conv2: dim → dim/2。可尝试改为 dim→dim/2.5（取整）或 dim→⌊0.4*dim⌋，最后融合 conv 也相应调整。
- 需要改源码，侵入性高；提升/风险比不如前几项；优先级低。
- 如果做：建议只在高维 stage (320,512) 做，低维保持原状。

6) RPN Head / Anchor Head

- 旋转检测对 RPN Recall 敏感，不建议率先裁剪通道或减少 anchors；收益小于风险，除非后期蒸馏弥补。

## 2. 不推荐或谨慎

- PatchEmbed 下采样卷积：参数占比小（相对大 Block），改动会影响特征分布与预训练映射，不划算。
- Depthwise 大核 (5×5,7×7 dilation) 直接删或统一成 3×3：会伤害 LSK 设计核心。尽量保留。
- 直接裁剪输出 stage 通道（64/128/320/512）：需要全链路(FPN/RPN/ROI)同步修改 + 预训练失配。若真需要，可放在最后一个迭代做统一 channel slimming。

## 3. 组合方案示例与可达比例

方案 A（最小侵入）：

- fc_out 1024→768
- mlp_ratios [8,8,4,4]→[6,6,3,3]
  预估节省：~9% + ~5~7% ≈ 14~16%（满足 10~20% 目标下限至中段）

方案 B（更激进）：

- fc_out 1024→640（≈14%）
- mlp_ratios [8,8,4,4]→[6,6,3,3]（+5~7%）
- 再把 FPN out_channels 256→224（额外 ~ (224/256)^2 ≈ 23% 对 fc1 输入部分缩减 + FPN/RPN conv 缩减；整网再降 4~6%）
  合计：可达 ~20%+（满足上限）

方案 C（在 B 基础上再轻量化 LSKblock 分流）

- 只在 stage3/4 将 conv1/conv2 的 dim/2 改成 dim/3
- 额外小幅 2~3%
  合计：>22%（需更谨慎验证精度）

## 4. 实施优先顺序建议

1. 改 ROI Head fc_out_channels + 重新训练或微调（最安全高收益）。
2. 调整 mlp_ratio（写兼容加载逻辑，strict=False），微调恢复。
3. 若仍需：降 ROI Head 为单 FC / Conv Head。
4. 再需：统一降低 FPN out_channels 并同步各 head。
5. 最后（可选）：修改 LSKblock 内部分流宽度。

## 5. 验证与回退策略

- 每一步单独改动后：统计参数 → 快速 fine-tune 2~3 epoch 验证 mAP 回落幅度；若 >2~3 mAP 再加入蒸馏（教师为原模型）。
- 蒸馏信号：
  - Backbone feature L2
  - RPN logits + proposals IoU 蒸馏
  - ROI Head 分类 logits + 回归分布（平滑 L1 / KL）
- 若一次性叠加多项剪枝，必须先单项 ablation 获取单项损失曲线，避免不可逆调参浪费。

## 6. 粗略参数节省估算公式（供快速评估）

- ROI Head 两 FC： (C_in*7*7*Fc + Fc^2) 主体；线性缩放近似：P_new / P_old ≈ (Fc_new / Fc_old) (主项)
- MLP 每 Block：P ≈ 2*D^2*ratio + 2*D^2 (注意力 1×1) + 常数项；降低 ratio 按线性缩放主项 2*D^2*Δratio
- FPN 侧分支 1×1：Σ (Cin_i * Cout)，统一减系数 k ⇒ 线性缩放
- RoI FC1 输入受 FPN out_channels 线性影响，进一步放大收益

## 7. 是否能达到 10%~20%

结论：可以。仅通过 ROI Head 降维 + MLP ratio 下调即可达到 14% 左右；再叠加轻度 FPN 降通道即可跨越 20%。无需触及 LSK 核心大卷积即可完成。

## 8. 下一步可执行行动（如需我继续可直接说）

- 生成一个剪枝版配置副本（例如 configs/lsknet/pruned/lsk_s_fpn_1x_dota_le90_pruneA.py）
- 写一个统计脚本对比参数
- 添加自定义加载逻辑 strict=False

如果需要我直接生成剪枝配置或脚本，请告诉我要采用哪个方案（A/B/自定义比例）。我可以马上改。
